{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "from abc import ABCMeta\n",
    "from fastparquet import ParquetFile\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from typing import Dict, List, Tuple\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.lib.io.tf_record import TFRecordWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "\n",
    "class LucyNERModelOutputSequenceInfo:\n",
    "\n",
    "    __output_sequence_info = {\n",
    "        1: \"-\", 2: \"B_CV\", 3: \"I_CV\", 4: \"I_OG\", 5: \"I_QT\", 6: \"I_PS\", 7: \"B_PS\", 8: \"B_LC\", 9: \"B_OG\", 10: \"B_QT\",\n",
    "        11: \"I_AF\", 12: \"I_DT\", 13: \"B_DT\", 14: \"I_TM\", 15: \"I_LC\", 16: \"B_AF\", 17: \"B_TM\", 18: \"B_AM\", 19: \"I_EV\",\n",
    "        20: \"I_TI\", 21: \"B_TI\", 22: \"B_EV\", 23: \"I_AM\", 24: \"B_PT\", 25: \"I_PT\", 26: \"B_FD\", 27: \"B_MT\", 28: \"I_TR\",\n",
    "        29: \"I_MT\", 30: \"B_TR\", 31: \"I_FD\", 0: \"PAD\"\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def get_output_sequence_info(cls) -> Dict[int, str]:\n",
    "        return cls.__output_sequence_info.copy()\n",
    "\n",
    "\n",
    "class LucyNERLabelInfo:\n",
    "\n",
    "    __label_infos = {\"PS\": {\"name\": \"PERSON\", \"child_labels\": [\"PS_NAME\", \"PS_CHARACTER\", \"PS_PET\"]}, \"FD\": {\"name\": \"STUDY_FIELD\", \"child_labels\": [\"FD_SCIENCE\", \"FD_SOCIAL_SCIENCE\", \"FD_MEDICINE\", \"FD_ART\", \"FD_HUMANITIES\", \"FD_OTHERS\"]}, \"TR\": {\"name\": \"THEORY\", \"child_labels\": [\"TR_SCIENCE\", \"TR_SOCIAL_SCIENCE\", \"TR_MEDICINE\", \"TR_ART\", \"TR_HUMANITIES\", \"TR_OTHERS\"]}, \"AF\": {\"name\": \"ARTIFACTS\", \"child_labels\": [\"AF_BUILDING\", \"AF_CULTURAL_ASSET\", \"AF_ROAD\", \"AF_TRANSPORT\", \"AF_MUSICAL_INSTRUMENT\", \"AF_WEAPON\", \"AFA_DOCUMENT\", \"AFA_PERFORMANCE\", \"AFA_VIDEO\", \"AFA_ART_CRAFT\", \"AFA_MUSIC\", \"AFW_SERVICE_PRODUCTS\", \"AFW_OTHER_PRODUCTS\"]}, \"OG\": {\"name\": \"ORGANIZATION\", \"child_labels\": [\"OGG_ECONOMY\", \"OGG_EDUCATION\", \"OGG_MILITARY\", \"OGG_MEDIA\", \"OGG_SPORTS\", \"OGG_ART\", \"OGG_MEDICINE\", \"OGG_RELIGION\", \"OGG_SCIENCE\", \"OGG_LIBRARY\", \"OGG_LAW\", \"OGG_POLITICS\", \"OGG_FOOD\", \"OGG_HOTEL\", \"OGG_OTHERS\"]}, \"LC\": {\"name\": \"LOCATION\", \"child_labels\": [\"LCP_COUNTRY\", \"LCP_PROVINCE\", \"LCP_COUNTY\", \"LCP_CITY\", \"LCP_CAPITALCITY\", \"LCG_RIVER\", \"LCG_OCEAN\", \"LCG_BAY\", \"LCG_MOUNTAIN\", \"LCG_ISLAND\", \"LCG_CONTINENT\", \"LC_SPACE\", \"LC_OTHERS\"]}, \"CV\": {\"name\": \"CIVILIZATION\", \"child_labels\": [\"CV_CULTURE\", \"CV_TRIBE\", \"CV_LANGUAGE\", \"CV_POLICY\", \"CV_LAW\", \"CV_CURRENCY\", \"CV_TAX\", \"CV_FUNDS\", \"CV_ART\", \"CV_SPORTS\", \"CV_SPORTS_POSITION\", \"CV_SPORTS_INST\", \"CV_PRIZE\", \"CV_RELATION\", \"CV_OCCUPATION\", \"CV_POSITION\", \"CV_FOOD\", \"CV_DRINK\", \"CV_FOOD_STYLE\", \"CV_CLOTHING\", \"CV_BUILDING_TYPE\"]}, \"DT\": {\"name\": \"DATE\", \"child_labels\": [\"DT_DURATION\", \"DT_DAY\", \"DT_WEEK\", \"DT_MONTH\", \"DT_YEAR\", \"DT_SEASON\", \"DT_GEOAGE\", \"DT_DYNASTY\", \"DT_OTHERS\"]}, \"TI\": {\"name\": \"TIME\", \"child_labels\": [\"TI_DURATION\", \"TI_HOUR\", \"TI_MINUTE\", \"TI_SECOND\", \"TI_OTHERS\"]}, \"QT\": {\"name\": \"QUANTITY\", \"child_labels\": [\"QT_AGE\", \"QT_SIZE\", \"QT_LENGTH\", \"QT_COUNT\", \"QT_MAN_COUNT\", \"QT_WEIGHT\", \"QT_PERCENTAGE\", \"QT_SPEED\", \"QT_TEMPERATURE\", \"QT_VOLUME\", \"QT_ORDER\", \"QT_PRICE\", \"QT_PHONE\", \"QT_SPORTS\", \"QT_CHANNEL\", \"QT_ALBUM\", \"QT_ADDRESS\", \"QT_OTHERS\"]}, \"EV\": {\"name\": \"EVENT\", \"child_labels\": [\"EV_ACTIVITY\", \"EV_WAR_REVOLUTION\", \"EV_SPORTS\", \"EV_FESTIVAL\", \"EV_OTHERS\"]}, \"AM\": {\"name\": \"ANIMAL\", \"child_labels\": [\"AM_INSECT\", \"AM_BIRD\", \"AM_FISH\", \"AM_MAMMALIA\", \"AM_AMPHIBIA\", \"AM_REPTILIA\", \"AM_TYPE\", \"AM_PART\", \"AM_OTHERS\"]}, \"PT\": {\"name\": \"PLANT\", \"child_labels\": [\"PT_FRUIT\", \"PT_FLOWER\", \"PT_TREE\", \"PT_GRASS\", \"PT_TYPE\", \"PT_PART\", \"PT_OTHERS\"]}, \"MT\": {\"name\": \"MATERIAL\", \"child_labels\": [\"MT_ELEMENT\", \"MT_METAL\", \"MT_ROCK\", \"MT_CHEMICAL\"]}, \"TM\": {\"name\": \"TERM\", \"child_labels\": [\"TM_COLOR\", \"TM_DIRECTION\", \"TM_CLIMATE\", \"TM_SHAPE\", \"TM_CELL_TISSUE_ORGAN\", \"TMM_DISEASE\", \"TMM_DRUG\", \"TMI_HW\", \"TMI_SW\", \"TMI_SITE\", \"TMI_EMAIL\", \"TMI_MODEL\", \"TMI_SERVICE\", \"TMI_PROJECT\", \"TMIG_GENRE\", \"TM_SPORTS\"]}}\n",
    "\n",
    "    @classmethod\n",
    "    def get_label_name_info(cls) -> Dict[str, str]:\n",
    "\n",
    "        return {label: cls.__label_infos[label].name for label in cls.__label_infos.keys()}\n",
    "\n",
    "    @classmethod\n",
    "    def get_child_label_parent_info(cls) -> Dict[str, str]:\n",
    "\n",
    "        child_label_parent_info = {}\n",
    "\n",
    "        for label, info in cls.__label_infos.items():\n",
    "            for child_label in info[\"child_labels\"]:\n",
    "                child_label_parent_info[child_label] = label\n",
    "\n",
    "        return child_label_parent_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-multilingual-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union, Dict\n",
    "\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "\n",
    "class LucyNERFeatureParser:\n",
    "\n",
    "    def __init__(self, max_length: int):\n",
    "\n",
    "        self.__tokenizer: PreTrainedTokenizer = tokenizer\n",
    "\n",
    "        self.__max_length = max_length\n",
    "\n",
    "    def __call__(self, texts: Union[List[str], str]):\n",
    "        return self.featuring(texts)\n",
    "\n",
    "    def featuring(self, texts: Union[List[str], str]):\n",
    "\n",
    "        max_length = self.__max_length\n",
    "\n",
    "        model_input = self.__tokenizer(\n",
    "            texts,\n",
    "            return_tensors='np',\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "\n",
    "        return model_input\n",
    "\n",
    "    def parse_input_id_to_token(self, input_ids):\n",
    "        return self.__tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "    def get_id_token_vocab(self) -> Dict:\n",
    "        return self.__tokenizer.get_vocab()\n",
    "\n",
    "    def get_token_id_vocab(self) -> Dict:\n",
    "        return self.__tokenizer.ids_to_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_parser =LucyNERFeatureParser(max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([[   101,   9739,  11467,   9435,  33654,   9318,  17342,  30005,\n",
       "         11664,   9739,  11467,   9409,  10622,   9408, 119245,  27355,\n",
       "         11018,   9485,  18784,   9895,  29455,   9739,  42337,   9366,\n",
       "         22333,  12692,   9366,  23811,  58823,   8935, 119437,  38696,\n",
       "         48345,    119,    102,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0]]), 'token_type_ids': array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_parser.featuring(['책으로 세상을 바라보고 책으로 삶을 살찌우는 시간 티비 책방 북소리 북마스터 김혜집니다.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERSentenceDataFeatureAnalyzer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.max_length = max_length\n",
    "        self.__tokenizer = tokenizer\n",
    "        self.__child_label_parent_info = LucyNERLabelInfo.get_child_label_parent_info()\n",
    "        self.__model_feature_parser = LucyNERFeatureParser(max_length=max_length)\n",
    "        sequence_label_info = LucyNERModelOutputSequenceInfo.get_output_sequence_info()\n",
    "        self.label_sequence_info = {value: key for key, value in sequence_label_info.items()}\n",
    "\n",
    "    def featuring(self, sentence_info: Dict):\n",
    "        return self.__parse_sentence_info_to_feature(sentence_info)\n",
    "\n",
    "    def __parse_sentence_info_to_feature(self, sentence_info: Dict) -> Dict:\n",
    "        sentence_id = sentence_info['sentence_id']\n",
    "        sentence_text = sentence_info['sentence_text']\n",
    "        sentence_ne_infos = sentence_info['sentence_ne_infos']\n",
    "\n",
    "        tokens, token_labels, token_ner_ids \\\n",
    "            = self.__tokenize_sentence_and_ne_mapping(sentence_text, sentence_ne_infos)\n",
    "\n",
    "        # input featuring\n",
    "        input_ids, attention_mask, token_type_ids = self.__featuring_model_input(sentence_text)\n",
    "\n",
    "        # output featuring\n",
    "        ## 앞에 special token을 채우기\n",
    "        output_ids = self.__parse_label_to_sequence(token_labels)\n",
    "        output_ids_pad = [1] + output_ids + [1]\n",
    "        output_ids_pad = pad_sequences([output_ids_pad], maxlen=self.max_length, padding='post',\n",
    "                                          truncating='post')\n",
    "\n",
    "        output_ids_pad = output_ids_pad[0].tolist()\n",
    "\n",
    "        sentence_feature = {\n",
    "            \"sentence_id\": sentence_id,\n",
    "            \"tokens\": tokens,\n",
    "            \"token_labels\": token_labels,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"token_type_ids\": token_type_ids,\n",
    "            \"output_ids\": output_ids_pad,\n",
    "        }\n",
    "\n",
    "        return sentence_feature\n",
    "\n",
    "    def __tokenize_sentence_and_ne_mapping(self, sentence_text, sentence_ne_infos):\n",
    "        # char 별 ne 정보를 사전화\n",
    "        id_ne_vocab = {}\n",
    "        char_idx_tag_dict = {}\n",
    "\n",
    "        for sentence_ne_info in sentence_ne_infos:\n",
    "\n",
    "            id_ne_vocab[sentence_ne_info['id']] = sentence_ne_info\n",
    "\n",
    "            for char_idx in range(sentence_ne_info['begin'], sentence_ne_info['end']):\n",
    "                char_idx_tag_dict[char_idx] = sentence_ne_info\n",
    "\n",
    "        # 토큰별 ne id 정보를 mapping\n",
    "        tokenizer = self.__tokenizer\n",
    "        token_list = []\n",
    "        token_ner_id_list = []\n",
    "\n",
    "        s_char_idx = 0\n",
    "        for word in sentence_text.split(' '):\n",
    "            word_token_list = tokenizer.tokenize(word)\n",
    "            token_list.extend(word_token_list)\n",
    "\n",
    "            for token in word_token_list:\n",
    "\n",
    "                try:\n",
    "                    char_tag = char_idx_tag_dict[s_char_idx]\n",
    "                    tag_id = char_tag['id']\n",
    "                except KeyError:\n",
    "                    tag_id = 0\n",
    "\n",
    "                token_ner_id_list.append(tag_id)\n",
    "\n",
    "                s_char_idx += len(token.replace('##', ''))\n",
    "\n",
    "            # 공백 idx\n",
    "            s_char_idx += 1\n",
    "\n",
    "        # 토큰 label mapping\n",
    "        token_ner_list = []\n",
    "        before_id = 0\n",
    "        for token_ner_id in token_ner_id_list:\n",
    "\n",
    "            if token_ner_id == 0:\n",
    "                token_ner_list.append('-')\n",
    "                continue\n",
    "\n",
    "            id_tag = id_ne_vocab[token_ner_id]\n",
    "            label = id_tag['label']\n",
    "            label = self.__parse_label_child_to_parent(label)\n",
    "\n",
    "            if before_id == id_tag['id']:\n",
    "                label = 'I_' + label\n",
    "            else:\n",
    "                label = 'B_' + label\n",
    "\n",
    "            token_ner_list.append(label)\n",
    "\n",
    "            before_id = id_tag['id']\n",
    "\n",
    "        return token_list, token_ner_list, token_ner_id_list\n",
    "\n",
    "    def __featuring_model_input(self, sentence_text: str) -> Tuple[List[int] ,List[int] ,List[int]]:\n",
    "\n",
    "        model_input = self.__model_feature_parser.featuring(sentence_text)\n",
    "        input_ids = model_input[\"input_ids\"][0]\n",
    "        attention_mask = model_input[\"attention_mask\"][0]\n",
    "        token_type_ids = model_input[\"token_type_ids\"][0]\n",
    "\n",
    "        return input_ids.tolist(), attention_mask.tolist(), token_type_ids.tolist()\n",
    "\n",
    "    def __parse_label_to_sequence(self, labels: List[str]) -> List[int]:\n",
    "\n",
    "        label_sequence_info = self.label_sequence_info\n",
    "\n",
    "        sequences = [label_sequence_info[label] for label in labels]\n",
    "\n",
    "        return sequences\n",
    "\n",
    "    def __parse_label_child_to_parent(self, label: str) -> str:\n",
    "        return self.__child_label_parent_info[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERFeaturingDataLoader(metaclass=ABCMeta):\n",
    "\n",
    "    def __init__(self, output_dir_path: str):\n",
    "\n",
    "        self.__create_path_dir(output_dir_path)\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        pass\n",
    "\n",
    "    def load(self):\n",
    "        pass\n",
    "\n",
    "    def __create_path_dir(self, output_dir_path: str):\n",
    "        train_data_dir_path = os.path.join(output_dir_path)\n",
    "        if not os.path.exists(train_data_dir_path):\n",
    "            os.makedirs(train_data_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERFeaturingDataToParquetIDXLoader(NERFeaturingDataLoader):\n",
    "\n",
    "    def __init__(self, output_dir_path: str):\n",
    "        super().__init__(output_dir_path)\n",
    "        self.output_dir_path = output_dir_path\n",
    "\n",
    "        self.file_idx = 0\n",
    "\n",
    "    def load(self, data: pd.DataFrame):\n",
    "        self.__load_parquet_file(data)\n",
    "        self.file_idx += 1\n",
    "\n",
    "    def __load_parquet_file(self, data: pd.DataFrame):\n",
    "        data_path = os.path.join(self.output_dir_path, str(self.file_idx) + \".parquet\")\n",
    "        data.to_parquet(data_path, engine=\"fastparquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERFeaturingDataToTFRecordLoader(NERFeaturingDataLoader):\n",
    "\n",
    "    def __init__(self, output_dir_path: str):\n",
    "        super().__init__(output_dir_path)\n",
    "\n",
    "        self.tfrecord_writer = TFRecordWriter(os.path.join(output_dir_path, \"dataset.tfrecord\"))\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        return self.tfrecord_writer.close()\n",
    "\n",
    "    def load(self, data: pd.DataFrame):\n",
    "        self.__load_tfrecord(data)\n",
    "\n",
    "    def __load_tfrecord(self, data: pd.DataFrame):\n",
    "\n",
    "        input_ids = data['input_ids'].values.tolist()\n",
    "        attention_mask = data['attention_mask'].values.tolist()\n",
    "        token_type_ids = data['token_type_ids'].values.tolist()\n",
    "        output_ids = data['output_ids'].values.tolist()\n",
    "\n",
    "        for i in range(len(input_ids)):\n",
    "            feature = {\n",
    "                'input_ids': self.__int64_list_feature(input_ids[i]),\n",
    "                'attention_mask': self.__int64_list_feature(attention_mask[i]),\n",
    "                'token_type_ids': self.__int64_list_feature(token_type_ids[i]),\n",
    "                'output_ids': self.__int64_list_feature(output_ids[i]),\n",
    "            }\n",
    "\n",
    "            tf_record_example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "            self.tfrecord_writer.write(tf_record_example.SerializeToString())\n",
    "\n",
    "    def __int64_list_feature(self, value: List):\n",
    "        return tf.train.Feature(int64_list=tf.train.Int64List(value=value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSpliter:\n",
    "\n",
    "    @classmethod\n",
    "    def split(cls, *arg, test_size=0.1):\n",
    "        data1, data2 = train_test_split(\n",
    "            *arg,\n",
    "            shuffle=True, random_state=777, test_size=test_size\n",
    "        )\n",
    "\n",
    "        return data1, data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataFeaturingAndSplitTransfer:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.feature_analyzer = NERSentenceDataFeatureAnalyzer()\n",
    "\n",
    "    def etl(self, input_parquet_data_dir_path: str, output_dir_path: str, save_type=\"tfrecord\"):\n",
    "\n",
    "        train_data_loader, test_data_loader, valid_data_loader = self.__get_data_loaders(save_type, output_dir_path)\n",
    "\n",
    "        ner_data_files = ParquetFile(input_parquet_data_dir_path)\n",
    "        analysis_columns = ['sentence_id', 'sentence_text', 'sentence_ne_infos']\n",
    "\n",
    "        for ner_datas in ner_data_files.iter_row_groups():\n",
    "            ner_datas_by_analysis = ner_datas[analysis_columns]\n",
    "\n",
    "            featured_datas = [self.feature_analyzer.featuring(row) for idx, row in ner_datas_by_analysis.iterrows()]\n",
    "            featured_datas = pd.DataFrame(featured_datas)\n",
    "\n",
    "            train_datas, test_datas = DataSpliter.split(featured_datas, test_size=0.2)\n",
    "\n",
    "            train_datas, valid_datas = DataSpliter.split(train_datas, test_size=0.1)\n",
    "\n",
    "            train_data_loader.load(train_datas)\n",
    "            test_data_loader.load(test_datas)\n",
    "            valid_data_loader.load(valid_datas)\n",
    "\n",
    "        # train_data_loader.close()\n",
    "        # test_data_loader.close()\n",
    "        # valid_data_loader.close()\n",
    "\n",
    "    def __get_data_loaders(self, save_type, output_dir_path):\n",
    "        train_data_dir_path = os.path.join(output_dir_path, \"train_data\")\n",
    "        test_data_dir_path = os.path.join(output_dir_path, \"test_data\")\n",
    "        valid_data_dir_path = os.path.join(output_dir_path, \"valid_data\")\n",
    "\n",
    "        if save_type == \"tfrecord\":\n",
    "            train_data_loader = NERFeaturingDataToTFRecordLoader(train_data_dir_path)\n",
    "            test_data_loader = NERFeaturingDataToTFRecordLoader(test_data_dir_path)\n",
    "            valid_data_loader = NERFeaturingDataToTFRecordLoader(valid_data_dir_path)\n",
    "        elif save_type == \"parquet\":\n",
    "            train_data_loader = NERFeaturingDataToParquetIDXLoader(train_data_dir_path)\n",
    "            test_data_loader = NERFeaturingDataToParquetIDXLoader(test_data_dir_path)\n",
    "            valid_data_loader = NERFeaturingDataToParquetIDXLoader(valid_data_dir_path)\n",
    "\n",
    "        return train_data_loader, test_data_loader, valid_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer = NERDataFeaturingAndSplitTransfer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_parquet_dir_path = '/home/woohyun/NER/data/k_corpus/result/kor_ner_parquet/'\n",
    "output_dir_path = f'/home/woohyun/NER/data/k_corpus/featured_data/{model_name}'\n",
    "save_format = 'parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer.etl(extract_parquet_dir_path, output_dir_path, save_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
